-----

# dLLM-Factory

**dLLM-Factory** is a comprehensive project focused on Diffusion Large Language Models (dLLMs). It provides a full suite of implementation code for core modules, including Pre-training, Supervised Fine-tuning (SFT), Reinforcement Learning (RL), and Inference.

[](https://www.star-history.com/#maomaocun/dLLM-Factorye&Timeline)

-----

## ğŸ“– Project Introduction

This project is designed to offer researchers and developers an efficient and user-friendly platform for dLLM training and inference. It supports the entire workflow, from data preprocessing and model training to inference and deployment. The project features a clear and organized structure, making it easy for users to undertake secondary development and customization.

## âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ§  é¢„è®­ç»ƒï¼ˆPretrainï¼‰ï¼š** ä»é›¶å¼€å§‹è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚
  - æ”¯æŒæ•°æ®é›†ï¼š`SlimPajama`

***

- **ğŸ”§ æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼š** å°†é¢„è®­ç»ƒæ¨¡å‹é€‚é…åˆ°ç‰¹å®šä»»åŠ¡ã€‚
  - æ”¯æŒæ•°æ®é›†ï¼š`simplescaling-s1K`

***

- **ğŸ¤– å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼š** åˆ©ç”¨åé¦ˆè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚
  - æ”¯æŒæ–¹æ³•ï¼š`diff-grpo`

***

- **ğŸš€ æ¨ç†ï¼ˆInferenceï¼‰ï¼š** é«˜æ•ˆè¿è¡Œå·²è®­ç»ƒæ¨¡å‹ä»¥åº”ç”¨äºå®é™…åœºæ™¯ã€‚
  - æ”¯æŒåŠ é€Ÿï¼š`dLLM-cache`

***

- **ğŸ“ˆ Evaluation:** Comprehensive evaluation on various benchmarks.
  | Benchmark | LLaDA Support | Dream Support |
  | :--- | :---: | :---: |
  | **BBH** | âœ… | âœ… |
  | **GPQA** | âœ… | âœ… |
  | **GSM8K** | âœ… | âœ… |
  | **HumanEval** | âœ… | âœ… |
  | **Long Bench** | âœ… | - |
  | **MBPP** | âœ… | âœ… |
  | **Minerva Math**| âœ… | âœ… |
  | **MMLU** | âœ… | âœ… |
  | **MMLU Pro** | âœ… | âœ… |

***

## ğŸ“ TODO

- [ ] Expand dataset support for pretraining and SFT
- [ ] Integrate more RL algorithms and strategies
- [ ] Add more dLLM acceleration methods (e.g., quantization, pruning, etc.)
- [ ] Add more evaluation benchmarks and metrics
- [ ] Enhance user experience for deployment and customization
## ğŸ› ï¸ Usage

### Pretraining

Execute the following command to start Pretraining:

```sh
cd pretrain
bash run_pretrain.sh
```

### Supervised Fine-tuning (SFT)

Execute the following command to start supervised fine-tuning:

```sh
cd sft
accelerate launch --config_file ./config/accelerate/lora_config.yaml ./sft.py
```

### Reinforcement Learning (RL)

To begin reinforcement learning, run the provided script:

```sh
cd rl
bash examples/script/train_diffu_grpo.sh
```


### Evalution

Get the evaluation results by this command:

```sh
cd evaluation
bash scripts/Dream/run_Dream_bbh_base.sh
```

## ğŸ™ Acknowledgments

We extend our sincere gratitude to the following projects for their excellent work. The Reinforcement Learning code in this repository is modified based on their contributions:

  - **[d1](https://github.com/dllm-reasoning/d1):** ä¸€ä¸ªä¸“æ³¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ æ‰©å±•æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é¡¹ç›®ã€‚
  - **[dLLM-cache](https://github.com/maomaocun/dllm-cache):** ä¸€ä¸ªç”¨äºè‡ªé€‚åº”ç¼“å­˜åŠ é€ŸdLLMçš„å®ç°ï¼Œå·²é›†æˆåˆ°æœ¬ä»“åº“ä¸­ã€‚
  - **[TinyLlama](https://github.com/jzhang38/TinyLlama)  [SMDM](https://github.com/ML-GSAI/SMDM):** æœ¬é¡¹ç›®çš„é¢„è®­ç»ƒä»£ç å‚è€ƒäº†è¿™äº›ä»“åº“ï¼Œéå¸¸æ„Ÿè°¢ä»–ä»¬çš„è´¡çŒ®ã€‚

## ğŸ“– Citation
```
@misc{yangyicun2025dLLMFactory,
  title={dLLM-Factory: A Comprehensive Platform for Diffusion Large Language Models},
  author={Yang Yicun and Cheng Shuang and Liu Dawei and Bian Yihan and Zhang Yaojie},
  year={2025},
  url = {https://github.com/maomaocun/dllm-Factory}
}
```

## ğŸ“§ Contact

For any questions or collaboration inquiries, feel free to reach out at: [yangyicun187@gmail.com](mailto:yangyicun187@gmail.com)

## :star2: Star History
[![Star History Chart](https://api.star-history.com/svg?repos=maomaocun/dLLM-Factory&type=Timeline)](https://www.star-history.com/#maomaocun/dLLM-Factorye&Timeline)
